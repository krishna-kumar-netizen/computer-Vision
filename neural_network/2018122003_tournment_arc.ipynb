{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Krishna_Assgn.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0t8pL4KUDiG",
        "outputId": "b08f5dcd-1087-46cd-93b1-475d533637c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLj_9KbzRlxW"
      },
      "source": [
        "# Dataset download\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMlmGzGRU2qg"
      },
      "source": [
        "#!pip install aicrowd-cli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz419WUZJe4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e8186f-6ac2-4726-c389-ceaf6e766055"
      },
      "source": [
        "import os\n",
        "# os.getcwd()\n",
        "os.chdir('/content/drive/MyDrive/dataset_cv_Assignment5')\n",
        "os.getcwd()\n",
        "!unzip -q /content/drive/MyDrive/dataset_cv_Assignment5/train_images.zip "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace train_images/815c3e59f4.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZqN_BA9SD9p"
      },
      "source": [
        "os.getcwd()\n",
        "!unzip -q /content/drive/MyDrive/dataset_cv_Assignment5/test_images.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wOyi_hdSoYY"
      },
      "source": [
        "# Dataset handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FiFzqaCJsO4"
      },
      "source": [
        "mport pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for reading and displaying images\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# for creating validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "PATH_TRAINING='/content/drive/MyDrive/dataset_cv_Assignment5/train_images'\n",
        "PATH_TESTING='/content/drive/MyDrive/dataset_cv_Assignment5/test_images'\n",
        "TRAIN_CLASS_PATH='/content/drive/MyDrive/dataset_cv_Assignment5/train.csv'\n",
        "TEST_CLASS_PATH='/content/drive/MyDrive/dataset_cv_Assignment5/test.csv'\n",
        "#NUM_TRAINING=len(os.listdir(PATH_TRAINING))\n",
        "#NUM_TESTING=len(os.listdir(PATH_TESTING))\n",
        "SUBSET_TRAINING=1000\n",
        "SUBSET_TESTING=500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XElHb8axJsPG"
      },
      "source": [
        "class dataprep1(Dataset):\n",
        "    def __init__(self,data_list,data_dir = './',transform=None,train=True):\n",
        "        super().__init__()\n",
        "        self.data_list = data_list\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "    def __getitem__(self,item):\n",
        "        if self.train:\n",
        "            img_name,label = self.data_list.iloc[item]\n",
        "        else:\n",
        "            img_name = self.data_list.iloc[item]['ImageId']\n",
        "        img_path = os.path.join(self.data_dir,img_name)\n",
        "        img1 = cv2.imread(img_path,1)\n",
        "        img1 = cv2.resize(img,(256,256))\n",
        "        if self.transform is not None:\n",
        "            img1 = self.transform(img1)\n",
        "        if self.train:\n",
        "            return {\n",
        "              'gt' : img1,\n",
        "              'label' : torch.tensor(label)\n",
        "          }\n",
        "        else:\n",
        "            return {\n",
        "              'gt':img1\n",
        "          }\n",
        "          \n",
        "    def __len__(self):\n",
        "        return self.data_list.shape[0]\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKFEa8odK87N"
      },
      "source": [
        "\n",
        "batch = 64\n",
        "\n",
        "train = pd.read_csv(TRAIN_LABELS)\n",
        "\n",
        "num = train['ClassName'].value_counts()\n",
        "classes = train['ClassName'].unique()\n",
        "num_classes = len(classes)\n",
        "\n",
        "label_enc = preprocessing.LabelEncoder()\n",
        "targets = label_enc.fit_transform(train['ClassName'])\n",
        "ntrain = train\n",
        "ntrain['ClassName'] = targets\n",
        "\n",
        "train_trsf = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "data_train= dataprep1(data_list= ntrain,data_dir = TRAIN_PATH,transform = train_trsf)\n",
        "\n",
        "valid_size = 0.2\n",
        "num = train_data.__len__()\n",
        "# Dividing the indices for train and cross validation\n",
        "indices = list(range(num))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size*num))\n",
        "train_idx,valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "#Create Samplers\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "loadtrainer = DataLoader(data_train, batch_size = batch, sampler = train_sampler, num_workers=20)\n",
        "loadvalid = DataLoader(data_train, batch_size = batch, sampler = valid_sampler, num_workers=20)\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5) , (0.5,0.5,0.5))\n",
        "])\n",
        "test_path = PATH_TESTING\n",
        "test = pd.read_csv(TEST_LABELS)\n",
        "test_data = data_handler(data_list= test,data_dir = test_path,transform = transforms_test,train=False)\n",
        "\n",
        "testloader = DataLoader(test_data, batch_size=batch, shuffle=False, num_workers=30)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k7PzfiOJsPN"
      },
      "source": [
        "class conv1(nn.Module):\n",
        "    def __init__(self,channels_list,lkernels,strides_list,padding_list,pool_type = 'maxpool',bn = True, activation = 'relu'):\n",
        "        super(conv1,self).__init__()\n",
        "        \n",
        "        num_layers = len(channels_list) - 1\n",
        "        if(pool_type == 'maxpool'):   self.pool    = nn.MaxPool2d(2,2)\n",
        "        elif(pool_type == 'avgpool'): self.pool    = nn.AvgPool2d(2,2)\n",
        "        elif(pool_type == 'l1pool'):  self.pool    = nn.LPPool2d(1, 2, stride = 2)\n",
        "        elif(pool_type == 'l2pool'):  self.pool    = nn.LPPool2d(2, 2, stride = 2)\n",
        "        else: print('pool_type not supported')\n",
        "        self.convs = []\n",
        "        for i in range(num_layers):\n",
        "            tmp = [basic_block(channels_list[i],channels_list[i+1], kernel_size = lkernels[i], stride = strides_list[i], padding = padding_list[i], bn = bn, activation = activation),self.pool]\n",
        "            for layer in tmp:\n",
        "                self.convs.append(layer)\n",
        "        self.convs = nn.ModuleList(self.convs)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        for layer in self.convs:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class basic_block(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size = 3,stride = 1,padding = 0,bn = True, activation = 'relu'):\n",
        "        super(basic_block,self).__init__()\n",
        "        \n",
        "        self.bn        = bn \n",
        "        self.conv      = nn.Conv2d(in_channels=in_channels,kernel_size=kernel_size,out_channels=out_channels,stride=stride,padding=padding)\n",
        "        self.batchnorm = nn.BatchNorm2d(num_features=out_channels)\n",
        "        if(activation == 'relu'):\n",
        "            self.relu  = nn.ReLU()\n",
        "        elif(activation == 'elu'):\n",
        "            self.relu  = nn.ELU()\n",
        "        elif(activation == 'leaky_relu'):\n",
        "            self.relu  = nn.LeakyReLU(0.1)\n",
        "        else:\n",
        "            print('activation not supported')\n",
        "\n",
        "    def forward(self,x):\n",
        "        output = self.conv(x)\n",
        "        if(self.bn):\n",
        "            output = self.batchnorm(output)\n",
        "        output = self.relu(output)\n",
        "        return output\n",
        "\n",
        "def calc_ln_in(inp_size,out_channel,lkernels,strides_list,padding_list,pooling_kernel,pooling_stride):\n",
        "    for i in range(len(lkernels)):\n",
        "        inp_size = 1 + ((inp_size - lkernels[i] + 2*padding_list[i])//strides_list[i])\n",
        "        inp_size = 1 + ((inp_size - pooling_kernel)//pooling_stride)\n",
        "    if(inp_size < 1):\n",
        "        print('Error: Incorrect nn.linear input. Got in_features = 1. Adjust parameters list')\n",
        "    else:\n",
        "        return out_channel*inp_size**2\n",
        "    \n",
        "class ln_list(nn.Module):\n",
        "    def __init__(self,lin_list):\n",
        "        super(ln_list,self).__init__()\n",
        "        num_layers = len(lin_list) - 1\n",
        "        self.linears = []\n",
        "        for i in range(num_layers):\n",
        "            tmp = [nn.Linear(lin_list[i],lin_list[i+1]),nn.ReLU()]\n",
        "            if(i == num_layers - 1):\n",
        "                self.linears.append(nn.Linear(lin_list[i],lin_list[i+1]))\n",
        "                break\n",
        "            for layer in tmp:\n",
        "                self.linears.append(layer)\n",
        "        self.linears = nn.ModuleList(self.linears)\n",
        "\n",
        "    def forward(self,x):\n",
        "        for layer in self.linears:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "   \n",
        "class DNN(nn.Module):\n",
        "    def __init__(self,in_size = 32,channels_list = [3,16,32],lkernels = [3,5],strides_list = 1,padding_list = [1,0],lin_list = [512,100],pool_type = 'maxpool',bn = False, dropout = False, activation = 'relu'):\n",
        "        super(CNN, self).__init__()\n",
        "        self.pool_type = pool_type\n",
        "        self.d = dropout\n",
        "        if(dropout):\n",
        "            self.dropout   = nn.Dropout2d(p = 0.2)\n",
        "        \n",
        "        num_layers = len(channels_list) - 1\n",
        "        if(type(lkernels) == int): lkernels = [lkernels for _ in range(num_layers)]\n",
        "        if(type(strides_list) == int): strides_list = [strides_list for _ in range(num_layers)]\n",
        "        if(type(padding_list) == int): padding_list = [padding_list for _ in range(num_layers)]\n",
        "        \n",
        "        self.in_features = calc_ln_in(in_size,out_channel = channels_list[-1],lkernels = lkernels,strides_list = strides_list,padding_list = padding_list,pooling_kernel = 2,pooling_stride = 2)\n",
        "        self.convs = conv1(channels_list, lkernels = lkernels, strides_list = strides_list,padding_list = padding_list, pool_type = self.pool_type, bn = bn, activation = activation)\n",
        "        self.linears = ln_list([self.in_features,*lin_list])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        if(self.d):\n",
        "            x = self.dropout(x)\n",
        "        x = x.view(-1,self.in_features)\n",
        "        x = self.linears(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDdWVyNVJsPP"
      },
      "source": [
        "class loss1(nn.Module):\n",
        "    def forward(self,outputs,labels):\n",
        "        if(self.loss_type == 'l1' or self.loss_type == 'l2'):\n",
        "            onh = torch.FloatTensor(len(labels),self.num_classes).to(device)\n",
        "            onh.zero_()\n",
        "            onh.scatter_(1,labels.view(len(labels),1),1)\n",
        "            arr = onh - F.softmax(outputs,dim=1)\n",
        "            return torch.sum(torch.abs(arr).pow(self.p)/len(labels))\n",
        "        elif(self.loss_type == 'cross_entropy'):\n",
        "            return self.criterion(outputs,labels)\n",
        "    \n",
        "    def __init__(self,loss_type = 'l2', num_classes=100):\n",
        "        super(loss1,self).__init__()\n",
        "        self.loss_type = loss_type\n",
        "        self.num_classes = num_classes\n",
        "        if(self.loss_type == 'l1'):\n",
        "            self.p = 1\n",
        "        elif(self.loss_type == 'l2'):\n",
        "            self.p = 2\n",
        "        elif(self.loss_type == 'cross_entropy'):\n",
        "            self.criterion = nn.CrossEntropyLoss()\n",
        "          \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzskdJuBJsPP"
      },
      "source": [
        "def acc(labels,out):\n",
        "    pred = np.argmax(out.detach().cpu().numpy(),axis=1)\n",
        "    return 100*accuracy_score(labels.cpu().numpy(),pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfO6znGN-46w"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FW072-CIJsPP"
      },
      "source": [
        "def train(in_size = 32,channels_list = [3,16,32],lkernels = [3,5],strides_list = 1,padding_list = [1,0],lin_list = [512,100],pool_type = 'maxpool',bn = False, dropout = False,activation = 'relu',loss_type = 'cross_entropy',num_epochs = 10):\n",
        "    model = DNN(in_size = in_size,channels_list = channels_list,lkernels = lkernels,strides_list = strides_list,padding_list = padding_list,lin_list = lin_list,pool_type = pool_type,bn = bn, dropout = dropout, activation=activation).to(device)\n",
        "    model = nn.DataParallel(model)\n",
        "    criterion = loss1(loss_type=loss_type, num_classes=num_classes)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=0.0001)\n",
        "    train_acc,test_acc = [],[]\n",
        "    train_loss,test_loss = [],[]\n",
        "    for epoch in range(num_epochs): \n",
        "\n",
        "        running_loss = 0.0\n",
        "        acc_list = []\n",
        "        model.train()\n",
        "        for i, data in enumerate(loadtrainer, 0):\n",
        "            inputs, labels = data['gt'].squeeze(0).to(device), data['label'].long().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            acc_list.append(acc(labels,outputs))\n",
        "\n",
        "        print('Train : [%d] loss: %.3f Accuracy: %.2f' %(epoch , running_loss / i, np.mean(np.array(acc_list))))\n",
        "\n",
        "        train_loss.append(running_loss/i)\n",
        "        train_acc.append(np.mean(np.array(acc_list)))\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        acc_list = []\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(loadvalid, 0):\n",
        "                inputs, labels = data['gt'].squeeze(0).to(device), data['label'].long().to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                acc_list.append(acc(labels,outputs))\n",
        "\n",
        "        print('Val : [%d] loss: %.3f Accuracy: %.2f' %(epoch , running_loss / i, np.mean(np.array(acc_list))))\n",
        "        test_loss.append(running_loss/i)\n",
        "        test_acc.append(np.mean(np.array(acc_list)))\n",
        "    \n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for images in testloader:\n",
        "            data = images['gt'].squeeze(0).to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pr = predicted.detach().cpu().numpy()\n",
        "            for i in pr:\n",
        "                preds.append(i)\n",
        "    \n",
        "    df = pd.DataFrame(label_enc.inverse_transform(preds),columns=['ClassName'])\n",
        "    bn_str = 'bn' if bn else ''\n",
        "    dropout_str = 'dropout' if dropout else ''\n",
        "    output_str = 'submissionKK_' + '_'.join([loss_type,bn_str,pool_type]) + '.csv'\n",
        "    df.to_csv(output_str,index=False)\n",
        "\n",
        "    return model,train_loss,test_loss,train_acc,test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbR_B8OPJsPS"
      },
      "source": [
        "def plot(train_loss,test_loss,train_acc,test_acc,num_epochs,title = ''):\n",
        "    x = np.arange(num_epochs)\n",
        "    fig = plt.figure()\n",
        "    ax = plt.subplot(111)\n",
        "    ax.plot(x, train_loss,'r',label='train_loss')\n",
        "    ax.plot(x, test_loss,'b',label='val_loss')\n",
        "    plt.xticks(x[::2])\n",
        "    plt.title('Loss curve' + str(' ') + title)\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "    fig1 = plt.figure()\n",
        "    ax = plt.subplot(111)\n",
        "    ax.plot(x, train_acc,'r',label='train_acc')\n",
        "    ax.plot(x, test_acc,'b',label='val_acc')\n",
        "    plt.xticks(x[::2])\n",
        "    plt.title('Accuracy curve' + str(' ') + title)\n",
        "    ax.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkevZR-j4Qvv"
      },
      "source": [
        "in_size = 256\n",
        "padding_list = [1,0,0]\n",
        "pool_type = 'maxpool'\n",
        "channels_list = [3,6,32,64]\n",
        "lkernels = [3,5,3]\n",
        "strides_list = 1\n",
        "dropout = True\n",
        "num_epochs = 20\n",
        "lin_list = [512,num_classes]\n",
        "bn = True\n",
        "activation = 'relu'\n",
        "loss_type = 'cross_entropy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_rtOy2LRwiX",
        "outputId": "5d2e49a8-a7ab-4fdc-b405-a5776761d316"
      },
      "source": [
        "_,train_loss,test_loss,train_acc,test_acc = train(in_size,channels_list,lkernels,strides_list,padding_list,lin_list,pool_type,bn,dropout,activation,loss_type,num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 30 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train : [0] loss: 9.383 Accuracy: 8.02\n",
            "Val : [0] loss: 4.033 Accuracy: 13.40\n",
            "Train : [1] loss: 3.661 Accuracy: 15.55\n",
            "Val : [1] loss: 3.755 Accuracy: 17.13\n",
            "Train : [2] loss: 3.425 Accuracy: 19.64\n",
            "Val : [2] loss: 3.615 Accuracy: 20.35\n",
            "Train : [3] loss: 3.238 Accuracy: 22.68\n",
            "Val : [3] loss: 3.524 Accuracy: 21.28\n",
            "Train : [4] loss: 3.083 Accuracy: 24.68\n",
            "Val : [4] loss: 3.434 Accuracy: 21.50\n",
            "Train : [5] loss: 2.948 Accuracy: 26.55\n",
            "Val : [5] loss: 3.413 Accuracy: 22.89\n",
            "Train : [6] loss: 2.833 Accuracy: 29.04\n",
            "Val : [6] loss: 3.445 Accuracy: 21.39\n",
            "Train : [7] loss: 2.689 Accuracy: 31.35\n",
            "Val : [7] loss: 3.404 Accuracy: 22.10\n",
            "Train : [8] loss: 2.584 Accuracy: 32.52\n",
            "Val : [8] loss: 3.391 Accuracy: 22.64\n",
            "Train : [9] loss: 2.474 Accuracy: 35.05\n",
            "Val : [9] loss: 3.504 Accuracy: 22.30\n",
            "Train : [10] loss: 2.326 Accuracy: 37.46\n",
            "Val : [10] loss: 3.476 Accuracy: 22.23\n",
            "Train : [11] loss: 2.178 Accuracy: 40.43\n",
            "Val : [11] loss: 3.557 Accuracy: 22.82\n",
            "Train : [12] loss: 2.023 Accuracy: 44.71\n",
            "Val : [12] loss: 3.535 Accuracy: 24.76\n",
            "Train : [13] loss: 1.852 Accuracy: 48.59\n",
            "Val : [13] loss: 3.642 Accuracy: 23.12\n",
            "Train : [14] loss: 1.729 Accuracy: 51.69\n",
            "Val : [14] loss: 3.728 Accuracy: 21.59\n",
            "Train : [15] loss: 1.568 Accuracy: 55.78\n",
            "Val : [15] loss: 3.884 Accuracy: 21.83\n",
            "Train : [16] loss: 1.399 Accuracy: 60.22\n",
            "Val : [16] loss: 4.078 Accuracy: 20.75\n",
            "Train : [17] loss: 1.259 Accuracy: 63.89\n",
            "Val : [17] loss: 4.098 Accuracy: 20.44\n",
            "Train : [18] loss: 1.077 Accuracy: 69.00\n",
            "Val : [18] loss: 4.388 Accuracy: 21.17\n",
            "Train : [19] loss: 0.920 Accuracy: 73.89\n",
            "Val : [19] loss: 4.574 Accuracy: 20.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Rs9mA_661s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPoIjWsjAqFe"
      },
      "source": [
        "Convoltion network with 4 convolution layers and fully connected dense layer for classificaiton used. This modelis trained with batch size of 64 and 20 epochs. Observed training loss is reducing but, validation loss is not reducing in a similar manner. Final validation accuracy of 20% is achieved. "
      ]
    }
  ]
}